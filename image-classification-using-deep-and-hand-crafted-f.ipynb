{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/tanishksarode/image-classification-using-deep-and-hand-crafted-f?scriptVersionId=232853988\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"1eaf3305","metadata":{"papermill":{"duration":0.002259,"end_time":"2025-04-09T11:57:39.237232","exception":false,"start_time":"2025-04-09T11:57:39.234973","status":"completed"},"tags":[]},"source":["# Image Classification using Deep and Hand-Crafted Feature Modeling\n","1. Extract deep features from the pre-trained VGG-16 model and extract hand crafted\n","Histogram Oriented Gradient (HOG) features for MNIST dataset. Stack the deep\n","features with HOG features and model it using a random forest classifier to classify\n","the MNIST dataset. Run the hybrid model 5 times and compute the mean accuracy.\n","2. Extract deep features from the pre-trained VGG-16 model and extract hand crafted\n","Scale Invariant Feature Transform (SIFT) features for MNIST dataset. Stack the deep\n","features with SIFT features and model it using a random forest classifier to classify\n","the MNIST dataset. Run the hybrid model 5 times and compute the mean accuracy.\n","3. Extract deep features from the pre-trained VGG-16 model and extract hand crafted\n","SIFT and HOG features for MNIST dataset. Stack the deep features with HOG and\n","SIFT features and model it using a random forest classifier to classify the MNIST\n","dataset. Run the hybrid model 5 times and compute the mean accuracy.\n","4. Extract deep features from the pre-trained VGG-16 model and extract hand crafted\n","SIFT and HOG features for MNIST dataset. Stack the deep features with HOG and\n","SIFT features and use PCA to transform and reduce the dimension (Test using\n","different component values). Model the transformed features using a random forest\n","classifier to classify the MNIST dataset. Run the hybrid model 5 times and compute\n","the mean accuracy.\n","5. Draw conclusions on the best model among the above four models for classifying\n","MNIST dataset."]},{"cell_type":"code","execution_count":1,"id":"caa353b7","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-04-09T11:57:39.241941Z","iopub.status.busy":"2025-04-09T11:57:39.241687Z","iopub.status.idle":"2025-04-09T11:57:53.141391Z","shell.execute_reply":"2025-04-09T11:57:53.14044Z"},"papermill":{"duration":13.90375,"end_time":"2025-04-09T11:57:53.143096","exception":false,"start_time":"2025-04-09T11:57:39.239346","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"]}],"source":["from tensorflow.keras.datasets import mnist\n","import cv2\n","import numpy as np\n","\n","\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","(X_train, y_train), (X_test, y_test) = (X_train[:500], y_train[:500]), (X_test[:100], y_test[:100])\n","\n","X_train_resized = np.array([cv2.resize(cv2.cvtColor(img, cv2.COLOR_GRAY2RGB), (224, 224)) for img in X_train])\n","X_test_resized = np.array([cv2.resize(cv2.cvtColor(img, cv2.COLOR_GRAY2RGB), (224, 224)) for img in X_test])\n","\n"]},{"cell_type":"code","execution_count":2,"id":"a23b7bfa","metadata":{"execution":{"iopub.execute_input":"2025-04-09T11:57:53.148712Z","iopub.status.busy":"2025-04-09T11:57:53.148224Z","iopub.status.idle":"2025-04-09T11:58:22.685203Z","shell.execute_reply":"2025-04-09T11:58:22.684369Z"},"papermill":{"duration":29.541041,"end_time":"2025-04-09T11:58:22.686644","exception":false,"start_time":"2025-04-09T11:57:53.145603","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 723ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step\n"]}],"source":["from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.models import Model\n","\n","# Load VGG-16\n","vgg16 = VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n","\n","# Create a feature extractor model\n","model = Model(inputs=vgg16.input, outputs=vgg16.output)\n","\n","# Extract features\n","deep_train_features = model.predict(X_train_resized)\n","deep_test_features = model.predict(X_test_resized)\n","\n","# Flatten features\n","deep_train_features = deep_train_features.reshape(len(X_train), -1)\n","deep_test_features = deep_test_features.reshape(len(X_test), -1)\n"]},{"cell_type":"code","execution_count":3,"id":"73be2655","metadata":{"execution":{"iopub.execute_input":"2025-04-09T11:58:22.695679Z","iopub.status.busy":"2025-04-09T11:58:22.695436Z","iopub.status.idle":"2025-04-09T11:58:22.833928Z","shell.execute_reply":"2025-04-09T11:58:22.833304Z"},"papermill":{"duration":0.143721,"end_time":"2025-04-09T11:58:22.835162","exception":false,"start_time":"2025-04-09T11:58:22.691441","status":"completed"},"tags":[]},"outputs":[],"source":["from skimage.feature import hog\n","\n","def extract_hog_features(images):\n","    hog_features = [hog(img, pixels_per_cell=(8, 8), cells_per_block=(2, 2), feature_vector=True) for img in images]\n","    return np.array(hog_features)\n","\n","hog_train_features = extract_hog_features(X_train)\n","hog_test_features = extract_hog_features(X_test)\n"]},{"cell_type":"code","execution_count":4,"id":"2a3750a1","metadata":{"execution":{"iopub.execute_input":"2025-04-09T11:58:22.843189Z","iopub.status.busy":"2025-04-09T11:58:22.842666Z","iopub.status.idle":"2025-04-09T11:58:26.29552Z","shell.execute_reply":"2025-04-09T11:58:26.2948Z"},"papermill":{"duration":3.458282,"end_time":"2025-04-09T11:58:26.297042","exception":false,"start_time":"2025-04-09T11:58:22.83876","status":"completed"},"tags":[]},"outputs":[],"source":["import cv2\n","from sklearn.cluster import KMeans\n","\n","sift = cv2.SIFT_create()\n","\n","def extract_sift_features(images, num_clusters=50):\n","    descriptors_list = []\n","    valid_indices = []  # Track indices of images with descriptors\n","\n","    for i, img in enumerate(images):\n","        keypoints, descriptors = sift.detectAndCompute(img, None)\n","        if descriptors is not None:\n","            descriptors_list.append(descriptors)\n","            valid_indices.append(i)  # Store valid image index\n","\n","    if len(descriptors_list) == 0:\n","        return np.zeros((len(images), num_clusters))  # Return zeros if no descriptors exist\n","\n","    # Stack descriptors and apply KMeans\n","    all_descriptors = np.vstack(descriptors_list)\n","    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n","    kmeans.fit(all_descriptors)\n","\n","    # Assign each image a histogram of cluster frequencies\n","    sift_features = []\n","    for descriptors in descriptors_list:\n","        labels = kmeans.predict(descriptors)\n","        hist = np.histogram(labels, bins=np.arange(num_clusters + 1))[0]\n","        sift_features.append(hist)\n","\n","    sift_features = np.array(sift_features)\n","\n","    # Handle missing images by adding zero vectors\n","    full_sift_features = np.zeros((len(images), num_clusters))\n","    full_sift_features[valid_indices] = sift_features  # Fill valid indices\n","\n","    return full_sift_features\n","\n","sift_train_features = extract_sift_features(X_train)\n","sift_test_features = extract_sift_features(X_test)\n"]},{"cell_type":"code","execution_count":5,"id":"e1615bb7","metadata":{"execution":{"iopub.execute_input":"2025-04-09T11:58:26.305171Z","iopub.status.busy":"2025-04-09T11:58:26.304706Z","iopub.status.idle":"2025-04-09T11:58:46.154679Z","shell.execute_reply":"2025-04-09T11:58:46.153722Z"},"papermill":{"duration":19.855472,"end_time":"2025-04-09T11:58:46.156234","exception":false,"start_time":"2025-04-09T11:58:26.300762","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean Accuracy (VGG + HOG): 0.9399999999999998\n","Mean Accuracy (VGG + SIFT): 0.9399999999999998\n","Mean Accuracy (VGG + HOG + SIFT): 0.9800000000000001\n","Mean Accuracy (VGG + HOG + SIFT + PCA): 0.97\n"]}],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","def train_and_evaluate(X_train, X_test, y_train, y_test):\n","    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","    rf.fit(X_train, y_train)\n","    y_pred = rf.predict(X_test)\n","    return accuracy_score(y_test, y_pred)\n","\n","# Model 1: VGG + HOG\n","X_train_1 = np.hstack((deep_train_features, hog_train_features))\n","X_test_1 = np.hstack((deep_test_features, hog_test_features))\n","acc_1 = train_and_evaluate(X_train_1, X_test_1, y_train, y_test)\n","\n","# Model 2: VGG + SIFT\n","X_train_2 = np.hstack((deep_train_features, sift_train_features))\n","X_test_2 = np.hstack((deep_test_features, sift_test_features))\n","acc_2 = train_and_evaluate(X_train_2, X_test_2, y_train, y_test)\n","\n","# Model 3: VGG + HOG + SIFT\n","X_train_3 = np.hstack((deep_train_features, hog_train_features, sift_train_features))\n","X_test_3 = np.hstack((deep_test_features, hog_test_features, sift_test_features))\n","acc_3 = train_and_evaluate(X_train_3, X_test_3, y_train, y_test)\n","\n","# Model 4: VGG + HOG + SIFT + PCA\n","from sklearn.decomposition import PCA\n","\n","pca = PCA(n_components=100)  # Tune this value\n","X_train_4 = pca.fit_transform(X_train_3)\n","X_test_4 = pca.transform(X_test_3)\n","acc_4 = train_and_evaluate(X_train_4, X_test_4, y_train, y_test)\n","\n","# Compute Mean Accuracy over 5 runs\n","import numpy as np\n","\n","def compute_mean_accuracy(X_train, X_test, y_train, y_test, runs=5):\n","    accuracies = [train_and_evaluate(X_train, X_test, y_train, y_test) for _ in range(runs)]\n","    return np.mean(accuracies)\n","\n","mean_acc_1 = compute_mean_accuracy(X_train_1, X_test_1, y_train, y_test)\n","mean_acc_2 = compute_mean_accuracy(X_train_2, X_test_2, y_train, y_test)\n","mean_acc_3 = compute_mean_accuracy(X_train_3, X_test_3, y_train, y_test)\n","mean_acc_4 = compute_mean_accuracy(X_train_4, X_test_4, y_train, y_test)\n","\n","# Print final results\n","print(f\"Mean Accuracy (VGG + HOG): {mean_acc_1}\")\n","print(f\"Mean Accuracy (VGG + SIFT): {mean_acc_2}\")\n","print(f\"Mean Accuracy (VGG + HOG + SIFT): {mean_acc_3}\")\n","print(f\"Mean Accuracy (VGG + HOG + SIFT + PCA): {mean_acc_4}\")\n"]},{"cell_type":"code","execution_count":null,"id":"bbaa613a","metadata":{"papermill":{"duration":0.003295,"end_time":"2025-04-09T11:58:46.16348","exception":false,"start_time":"2025-04-09T11:58:46.160185","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30918,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":71.344825,"end_time":"2025-04-09T11:58:47.887083","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-04-09T11:57:36.542258","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}